import asyncio
import json
import logging

from rica.adapters import transformers_adapter as tf
from rica.core.application import RiCA

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 1. Create the RiCA application instance first.
app = RiCA("demo.sys", description="System utilities for demo purposes")


# 2. Register tools with the application instance.
@app.route("/exec", background=False, timeout=5000)
async def _sys_python_exec(input_):
    """
    A tool to execute a single line of Python code safely.

    Input Schema:
    {
        "code": "string - Python expression to evaluate"
    }

    Output Schema:
    {
        "result": "string - Evaluation result",
        "error": "string - Error message (if any)"
    }

    WARNING: This is a demonstration. Do NOT use eval() in production.
    """
    try:
        code = input_.get("code", "")
        if not code:
            return {"error": "No code provided"}

        # Limit code length and content
        if len(code) > 100:
            return {"error": "Code too long (max 100 chars)"}

        # Use ast.literal_eval for safer evaluation of literals
        import ast
        result = ast.literal_eval(code)
        return {"result": str(result)}
    except (ValueError, SyntaxError) as e:
        return {"error": f"Invalid expression: {str(e)}"}
    except Exception as e:
        logger.error(f"Execution error: {e}", exc_info=True)
        return {"error": f"Execution failed: {str(e)}"}


# 3. Create the Executor, passing the app instance to it.
rt = tf.TransformersExecutor(model_name="Qwen/Qwen3-0.6B")


# 4. Register callbacks.
@rt.token_generated
def _on_token(token: str):
    # This function receives every single token generated by the model.
    # It represents the model's "thinking" process, including tool calls.
    print(token, end="", flush=True)


@rt.trigger
def _on_response(response_payload: any):
    # This function is ONLY called when the model uses the `rica.response` tool.
    # It's for showing the final, user-facing answer.
    print("\n\n--- USER-FACING RESPONSE ---")
    # Assuming the response payload is a list of content blocks, as per the prompt.
    if isinstance(response_payload, list):
        for item in response_payload:
            if item.get("type") == "text":
                print(item.get("content", ""))
    else:
        print(json.dumps(response_payload, indent=2))
    print("----------------------------\n")


async def main():
    print("--- Starting RiCA Demo ---")

    # 5. Start the reasoning thread. It will initialize the model and wait for input.
    # The `run()` call here is important to start the loop, even before first insert.
    await rt.initialize()
    await rt.install(app)
    rt.run()
    # Initially, it will pause itself waiting for input.

    # 6. Insert the initial user prompt. The adapter will format it correctly.
    await rt.insert(
        "Please calculate 123*456 using demo.sys/exec. "
        "Think step-by-step about what you need to do, and then use the "
        "rica.response tool to give me the final answer."
    )

    # 7. Wait for the generation to complete (e.g., hit EOS or pause after tool call).
    print(rt.context)
    await rt.wait()

    print("\n\n--- Final Context ---")
    print(rt.context)
    print("---------------------")

    # 8. Clean up resources.
    await rt.destroy()


if __name__ == "__main__":
    asyncio.run(main())
