import asyncio
import json
import re
from typing import Any, Callable, List, Optional
from xml.etree import ElementTree as ET

from .. import router
from ..exceptions import InvalidRiCAString
from ..server import CallBack, RiCA

__all__ = ["_ReasoningThreadTemplate"]


class _ReasoningThreadTemplate:
    """
    Base template for reasoning adapters.

    This class provides reusable, framework-agnostic helpers for:
    - Managing a textual context buffer.
    - Registering two types of callbacks:
      - @token_generated: Receives every raw token from the model.
      - @trigger: Receives only the payload from a `rica.response` tool call.
    - Detecting and executing <rica ...>...</rica> tool calls, with special
      handling for `rica.response`.
    """

    def __init__(self, app: RiCA, context: str = ""):
        """
        Initializes the reasoning thread template.

        Args:
            app: The RiCA application instance containing tool definitions.
            context: The initial context string.
        """
        if not isinstance(app, RiCA):
            raise TypeError("The 'app' argument must be an instance of RiCA.")
        self._app: RiCA = app
        self._context: str = context or ""
        self._response_callbacks: List[Callable[[Any], Any]] = []  # For @trigger
        self._token_callbacks: List[Callable[[str], Any]] = []  # For @token_generated

    # ---- Lifecycle placeholders (to be implemented by subclasses) ----
    async def insert(self, text: Any):
        """Insert external text into the context (to be implemented by subclass)."""
        raise NotImplementedError

    async def wait(self):
        """Wait for current reasoning/generation to complete (to be implemented)."""
        raise NotImplementedError

    async def destroy(self):
        """Stop and clean up resources (to be implemented)."""
        raise NotImplementedError

    def run(self):
        """Start or resume reasoning (to be implemented)."""
        raise NotImplementedError

    def pause(self):
        """Pause reasoning (to be implemented)."""
        raise NotImplementedError

    @property
    def context(self) -> str:
        """Return the current textual context buffer."""
        return self._context

    # ---- Common helpers ----
    def trigger(self, function: Callable[..., Any]) -> Callable[..., Any]:
        """
        Register a callback that will be called for `rica.response` tool calls.
        This is for delivering final responses to the user.
        """
        self._response_callbacks.append(function)
        return function

    def token_generated(self, function: Callable[[str], Any]) -> Callable[[str], Any]:
        """
        Register a callback that will be called for every token generated by the model.
        This is for observing the model's "thinking" process.
        """
        self._token_callbacks.append(function)
        return function

    async def _emit_response(self, payload: Any):
        """Emit a final response payload to all @trigger callbacks."""
        if not payload:
            return

        tasks = []
        for cb in self._response_callbacks:
            if asyncio.iscoroutinefunction(cb):
                tasks.append(asyncio.create_task(cb(payload)))
            else:
                # Wrap sync function in to_thread to avoid blocking
                tasks.append(asyncio.create_task(asyncio.to_thread(cb, payload)))

        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

    async def _emit_token(self, piece: str):
        """Emit a raw generated token to all @token_generated callbacks."""
        if not piece:
            return

        tasks = []
        for cb in self._token_callbacks:
            if asyncio.iscoroutinefunction(cb):
                tasks.append(asyncio.create_task(cb(piece)))
            else:
                # Wrap sync function in to_thread to avoid blocking
                tasks.append(asyncio.create_task(asyncio.to_thread(cb, piece)))

        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

    async def _detect_and_execute_tool_tail(self) -> tuple[bool, Optional[str]]:
        """
        Detect a trailing <rica ...>...</rica> in the current context, execute it
        via the configured RiCA app, append the result to the context, and emit it.
        Special handling for `rica.response` which triggers response callbacks instead.

        Returns:
            A tuple containing:
            - bool: True if a tool call was detected and handled.
            - Optional[str]: The text that was appended to the context as a result.
        """
        pattern = r"<rica\s+[^>]*>.*?<\/rica>(?!.*<rica\s+[^>]*>.*?<\/rica>)"
        match = re.search(pattern, self._context, re.DOTALL | re.IGNORECASE)

        if not match:
            return False, None

        tag_text = match.group(0)

        try:
            # Use a simple XML parser to robustly get attributes and content
            # A more robust solution would handle potential malformed XML from the LLM
            parser = ET.XMLParser(target=ET.TreeBuilder())
            # Wrap in a dummy root tag to handle multiple root elements or text nodes
            parser.feed(f"<root>{tag_text}</root>")
            root = parser.close()[0]  # Get the original <rica> tag

            package_name = root.attrib.get("package")
            if not package_name:
                raise InvalidRiCAString("Missing 'package' attribute in <rica> tag.")

            content_str = root.text or ""
            content = json.loads(content_str) if content_str.strip() else {}

            # --- Special handling for rica.response ---
            if package_name == "rica.response":
                await self._emit_response(content)
                # The response tool does not return anything to the context.
                # It's a terminal action for the user.
                return True, None

            # --- For all other tools ---
            # Re-use the original tag_text for decoding to preserve exact formatting
            application, data = router._decode(self._app, tag_text)
            result = await router._call(application, data)

            appended: str
            if isinstance(result, CallBack):
                payload = result.callback
                if isinstance(payload, (dict, list)):
                    appended = json.dumps(payload, ensure_ascii=False)
                else:
                    appended = str(payload)
            else:
                # It's a UUID for a background call
                appended = json.dumps({"call_id": str(result)}, ensure_ascii=False)

            self._context += appended
            await self._emit_token(appended)  # Emit the tool result as part of the thinking stream
            return True, appended
        except Exception as e:
            error_message = f"[tool-error]{type(e).__name__}: {e}"
            self._context += error_message
            await self._emit_token(error_message)
            return False, None
